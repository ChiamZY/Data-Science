{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generalization of a simple and intutive classifier called the maximal margin classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  8.1. Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. What Is a Hyperplane?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a p-dimensional space, a hyperplane is a flat affine subspace of dimension p - 1\n",
    "    * Divides the p-dimensional space into halves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2. Classification Using a Separating Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construct a hyperplane that seprates the training observations perfectly according to their class labels\n",
    "    * A test observation is assigned a class depending on which side of the hyperplane it is located\n",
    "    * The magnitude of the $f(x^{*})$ determines how far the point lies from the hyperplane\n",
    "        + The further $x^{*}$ lies from the hyperplane, the more confident our class assignment for $x^{*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. The Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maximal margin hyperplane\n",
    "    * The separating hyperplane that is farthest from the training observations\n",
    "        + We can then classify a test observation based on which side of the maximal margin hyperplane it lies also known as a maximal margin classifier\n",
    "    * The observations that are closest to the maximal margin hyperplane are known as support vectors\n",
    "        + The observations \"support\" the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.4. Construction of the Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maxmimal margin hyperplane is the solution to the optimization problem that maximizes M\n",
    "    * To ensure that each observation is on the correct side of the hyperplane and at least distance M from the hyperplane\n",
    "        + 2 additional constraints are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.5. The Non-separable Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In many cases, no separating hyperplane exists\n",
    "    * Need to extend the concept of a separating hyperplane in order to develop a hyperplane that almosts separates the classes, using a so-called soft margin\n",
    "        + Generalization of the maximal margin classifier to the no-separable case is known as the support vector classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Support Vector Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Overview of the Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A classifier based on a separating hyperplane will necessarily perfectly classify all of the training observations\n",
    "    * This will lead to sensitivity to individual observations\n",
    "    * Addition of a single observation in would lead to a dramatic change in the maximal margin hyperplane\n",
    "        + Distance of an observation form the hyperplane can be seen as a measure of our confidence that the observation was correctly classified\n",
    "        + Given that the maximal margin hyperplane is extremely sensitive to a change in a single observation suggest that it may have overfitted the training data\n",
    "        \n",
    "- A classifier based on a hyperplane that does not separate the two classes could provide\n",
    "    * Greater robustness to individual observations\n",
    "    * Better classification of most of the training observations\n",
    "\n",
    "- A support vector classifier has a soft margin which allows some of the training observations to be incorrectly classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. Details of the Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Support vector classifier:\n",
    "- C is a nonnegative tuning parameter\n",
    "    * Bounds the sum of the $\\epsilon_{i}$'s so it determines the number of severity of the violations to the margin that the hyperplane will tolerate\n",
    "        + If C = 0: Maximal margin hyperplane, no budget for violations\n",
    "        + If C > 0: No more than C observations can be on the wrong side of the hyperplane\n",
    "        + C is often chosen by cross-validation as it controls the bias-variance trade-off of the statistical learning technique\n",
    "- M is the width of the margin (as large as possible)\n",
    "- Slack variables $\\epsilon_{i}$ that allow individual observations to be on the wrong side of the margin or the hyperplane\n",
    "- Observations that either lie on the margin or that violate the margin will affect the hyperplane\n",
    "    * There are also known as support vectors\n",
    "        + When C is small, there will be fewer support vectors. This results in low bias but high variance.\n",
    "        + When C is large, there will be more support vectors. This reuslts in high bias, but low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1. Classification with Non-linear Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To address the problem of the support vector classifier, we could enlarge the feature space using quadratic, cubic and even higher order polynomial functions of the predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. The Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The support vector machines (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels\n",
    "- A kernel is a function that quantifies the similarity of two observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
