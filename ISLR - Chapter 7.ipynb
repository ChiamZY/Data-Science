{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 7\n",
    "\n",
    "- [7. Tree-Based Methods](#7.-Tree-Based-Methods)\n",
    "    * [7.1. The Basics of Decision Trees](#7.1.-The-Basics-of-Decision-Trees)\n",
    "        + [7.1.1. Regression Trees](#7.1.1.-Regression-Trees)\n",
    "        + [7.1.2. Classification Trees](#7.1.2.-Classification-Trees)\n",
    "        + [7.1.3. Trees Versus Linear Models](#7.1.3.-Trees-Versus-Linear-Models)\n",
    "        + [7.1.4. Advantages and Disadvantages of Trees](#7.1.4.-Advantages-and-Disadvantages-of-Trees)\n",
    "    * [7.2. Bagging, Random Forests, Boosting](#7.2.-Bagging,-Random-Forests,-Boosting)\n",
    "        + [7.2.1. Bagging](#7.2.1.-Bagging)\n",
    "        + [7.2.2. Random Forests](#7.2.2.-Random-Forests)\n",
    "        + [7.2.3. Boosting](#7.2.3.-Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Involves stratifying or segmenting the predictor space into a number of simple regions\n",
    "    * In order to make a prediction for a given observation, we typicallly use the mean or the mode of the training observations in the region to which it belongs\n",
    "- ***Bagging, random forests and boosting*** involves producing multiple trees which are then combined to yield a simple consensus prediction\n",
    "    * Dramatic improvement in prediction accuracy at the expense of some loss in interpreation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. The Basics of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1. Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees are typically drawn upside down\n",
    "    * Leaves at the bottom of the tree\n",
    "    * Points along the tree where the predictor space is split are referred to as internal nodes\n",
    "    * Each segment of the trees that connects the nodes are known as branches\n",
    "    \n",
    "**Prediction via Stratification of the Feature Space**\n",
    "- Divide the predictor space into J distinct and non-overlapping regions\n",
    "- For every observation that falls into the region, make the same prediction, which is simply the mean of the response values for the training observations\n",
    "    * Choose to divide the predictor space into into regions that minimize the RSS\n",
    "\n",
    "$$ \\sum^{J}_{j=1}\\sum_{i \\in R_{j}} (y_{i}-\\hat{y}_{R_{j}})^{2}$$\n",
    "where $\\hat{y}_{R_{j}}$ is the mean response for the training observations within the jth box.\n",
    "- Recursive binary splitting where each step of the tree building process accounts for the best split at that particular step rather than looking ahead and picking a split that lead to a better tree in some future step\n",
    "\n",
    "**Tree Pruning**\n",
    "- A smaller tree with fewer splits might lead to lower variance and better interpreation at the cost of a little bias\n",
    "    * A better strategy is to grow a very large tree $T_{0}$ and then prune it back to obtain a subtree\n",
    "        + Select a subtree that leads to the lowest test error rate\n",
    "- Cost complexity pruning\n",
    "    * Weakest link pruning; Consider a sequence of trees indexed by a non-negative tuning parameter $\\alpha$\n",
    "        + As $alpha$ increases, there is a price to pay for having a tree with many terminal nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For classification trees, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs\n",
    "    * We can use recursive binary splits to grow the classification tree\n",
    "- When building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split\n",
    "    * A Gini index is referred to as a measure of a node *purity*\n",
    "        + A small value indicates that a node contains predominantly observations from a single class\n",
    "    * Entropy is an alternative to Gini index\n",
    "$$ D = - \\sum^{K}_{k=1}\\hat{P}_{mk}log\\hat{p}_{mk}$$\n",
    "        + Entropy will take on a small value if the mth node is pure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3. Trees Versus Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression asssume a model of the form\n",
    "$$ f(X) = \\beta_{0} + \\sum^{p}_{j=1}X_{j}\\beta_{j} $$\n",
    "- Regression trees assume a model of the form\n",
    "$$ f(X) = \\sum^{M}_{m=1}C_{m}\\cdot 1_{(X \\in R_{m})} $$\n",
    "- If relationship between features and the response is well approximated by a linear model, then an approach such as a linear regression will likely work well and outperform a modethod such as a regression tree\n",
    "- If a highly non-linear and complex relationship between the features and the response exists, then decision trees may outperform classical approaches\n",
    "    * Trees may also be preferred for the sake of interpretability and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4. Advantages and Disadvantages of Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees for regression and classification have a number of advantages over the more classical approaches\n",
    "    * Advantages:\n",
    "        + Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\n",
    "        + Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches\n",
    "        + Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).\n",
    "        + Trees can easily handle qualitative predictors without the need to create dummy variables\n",
    "    * Disadvantages:\n",
    "        + Trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches\n",
    "        + Trees can be very non-robust. A small change in the data can cause a large change in the final estimated tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Bagging, Random Forests, Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bootstrap agreegation, or bagging is a general-purpose procedure for reducing the variance of a statistical learning method\n",
    "    * By building a separate prediction model using each training set and average the resulting predictions, we can obtain a single low-variance statistical learning model\n",
    "    * By taking repeated samples from the training dataset (bootstap), we can train our method and obtain the average predictions\n",
    "- Individual trees have high variance but low bias. \n",
    "    * Averaging these trees reduces the variance\n",
    "    \n",
    "**Out-of-Bag Error Estimation**\n",
    "- Each bagged tree makes use of around two-thirds of the observations\n",
    "    * The remaining one-third of the obervations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations\n",
    "        + We can predict the response for the ith observation using each of the trees in which that observation was OOB\n",
    "\n",
    "**Variable Importance Measures**\n",
    "- When we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree and it is no longer clear which variables are important to the procedure\n",
    "    * Bagging improves prediction accuracy at the expense of interpretability\n",
    "- An overall summary of the importance of each predictor using the RSS or the Gini index can be obtained\n",
    "    * A large value indicates an important predictor    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees\n",
    "    * When building these decision trees, a ran sample of *m* predictors is chosen as split candidates from the full set of *p* predictors\n",
    "        + Typically, $m \\approx \\sqrt{p}$\n",
    "\n",
    "- If there is a strong predictor, all the bagged trees will be highly correlated and will not lead to a substantial reduction in variance over a single tree\n",
    "    * By forcing each split to consider only a subset of the predictors, making the average of the resulting trees less variable and hence more reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In **boosting**, each decision tree is grown using information from previously grown trees\n",
    "    * Each tree is fit on a modified version of the original data set\n",
    "- Fitting a single large decision tree to the data amounts to fitting the data hard and potentially overfitting\n",
    "    * Boosting approach learns slowly and fits a decision tree to the residuals from the model\n",
    "        + This new decision tree is added to the fitted function to update the residuals\n",
    "- Boosting has three tuning parameters\n",
    "    * The number of trees ***B***\n",
    "        + Boosting can overfit if ***B*** is too large\n",
    "    * The shrinkage parameter $\\lambda$, which is a small positive number\n",
    "        + Controls the rate at which boosting learns.\n",
    "        + Typical values are 0.01 or 0.001\n",
    "        + Very small $\\lambda$ require a large value of ***B*** in order to acheive good performance\n",
    "    * The number of *d* of splits in each tree\n",
    "        + Controls the complexity of the boosted ensemble\n",
    "        + *d* is the interaction depth, and controls the interaction order of the boosted model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
