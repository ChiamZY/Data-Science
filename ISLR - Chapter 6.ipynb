{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 6\n",
    "\n",
    "- [6. Moving Beyond Linearity](#6.-Moving-Beyond-Linearity)\n",
    "    * [6.1. Polynomial Regression](#6.1.-Polynomial-Regression)\n",
    "    * [6.2. Step Functions](#6.2.-Step-Functions)\n",
    "    * [6.3. Basis Functions](#6.3.-Basis-Functions)\n",
    "    * [6.4. Regression Splines](#6.4.-Regression-Splines)\n",
    "        + [6.4.1. Piecewise Polynomials](#6.4.1.-Piecewise-Polynomials)\n",
    "        + [6.4.2. Constraints and Splines](#6.4.2.-Constraints-and-Splines)\n",
    "        + [6.4.3. The Spline Basis Representation](#6.4.3.-The-Spline-Basis-Representation)\n",
    "        + [6.4.4. Choosing the Number and Locations of the Knots](#6.4.4.-Choosing-the-Number-and-Locations-of-the-Knots)\n",
    "        + [6.4.5. Comparison to Polynomial Regression](#6.4.5.-Comparison-to-Polynomial-Regression)\n",
    "    * [6.5. Smoothing Splines](#6.5.-Smoothing-Splines)\n",
    "        + [6.5.1. An Overview of Smoothing Splines](#6.5.1.-An-Overview-of-Smoothing-Splines)\n",
    "        + [6.5.2. Choosing the Smoothing Parameter λ](#6.5.2.-Choosing-the-Smoothing-Parameter-λ)\n",
    "    * [6.6. Local Regression](#6.6.-Local-Regression)\n",
    "    * [6.7. Generalized Additive Models](#6.7.-Generalized-Additive-Models)\n",
    "        + [6.7.1 GAMs for Regression Problems](#6.7.1-GAMs-for-Regression-Problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Moving Beyond Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polynomial regression\n",
    "    * Extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power\n",
    "        + Simple way to provide a non-linear fit to data\n",
    "- Step functions\n",
    "    * Cut the range of a variables into $K$ distinct regions in order to produce a qualitative variable\n",
    "        + Fitting a piecewise constant function\n",
    "- Regression splines\n",
    "    * More flexible than polynomial and step functions\n",
    "    * Involves dividing the range of $X$ into $K$ distinct regions\n",
    "    * Within each region, a polynomial function is fitted to the data\n",
    "        + polynomials are constrained so they join smoothly at the region boundaries or knots\n",
    "- Smoothing splines\n",
    "    * Result from minimizing a residual sum of squares criterion subject to a smoothness penalty\n",
    "- Local regression\n",
    "    * Similar to splines but differs in an important way\n",
    "        + Regions are allowed to overlap and they do so in a smooth way\n",
    "- Generalized additive models\n",
    "    * Allows us to extend the methods above to deal with multiple predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} $$\n",
    "\n",
    "$$ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{i}^{2} + + \\beta_{3}x_{i}^{3} + ... + + \\beta_{d}x_{i}^{d} + \\epsilon_{i} $$ \n",
    "\n",
    "- Polynomial regression allows us to produce an extremely non-linear curve\n",
    "- Coefficients of the polynomial regression can be easily estimated using least squares linear regression because it is just a standard linear model with predictors $x_{i}$, $x_{i}^{2}$, $x_{i}^{3}$, ..., $x_{i}^{d}$\n",
    "- Least squares regression returns variance estimates for each of the fitted coefficients, $\\hat{\\beta_{j}}$ as well as the covariances between pairs of coefficient estimates\n",
    "    * The can be used to compute the estimated variance of $\\hat{f}(x_{0})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Step Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using polynomial functions of features as predictors in a linear model imposes a global structure on the non-linear function of $X$\n",
    "    * Using step functions can avoids imposing such a global structure\n",
    "- Step function breaks the range of $X$ into bins and fit a different constant in each bin\n",
    "    * Converting a continuous variable into an ordered categorical variable\n",
    "        + We can then use least squares to fit a linear model using $C_{1}(X), C_{2}(X), ... , C_{K}(X)$ as predictors\n",
    "\n",
    "$$y_{i} = \\beta_{0} + \\beta_{1}C_{1}(x_{i}) + ... + \\beta_{K}C_{K}(x_{i}) + \\epsilon_{i}$$\n",
    "where for a given value of X, at most one of the $C_{1}, C_{2}, ... , C_{K}$ can be non-zero\n",
    "\n",
    "- Unless there are natural breakpoints in the predictors, piecewise-constant functions can miss the trend in the previous bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Basis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polynomial and piecewise-constant regression models are in fact special cases of a basis function approach\n",
    "\n",
    "$$y_{i} = \\beta_{0} + \\beta_{1}b_{1}(x_{i}) + \\beta_{2}b_{2}(x_{i}) +... + \\beta_{K}b_{K}(x_{i}) + \\epsilon_{i}$$\n",
    "\n",
    "where the basis function $b_{1}(\\cdot), b_{2}(\\cdot),..., b_{K}(\\cdot)$ are fixed and known\n",
    "\n",
    "- For polynomial regression, the basis functions are $b_{j}(x_{i}) = x^{j}_{i}$ and for piecewise constant functions, they are $b_{j}(x_{i}) = I(c_{j} \\leq x_{i} < c_{j+1})$\n",
    "    * Similar to a standard linear model with predictors $\\beta_{1}(x_{i}), \\beta_{2}(x_{i}),..., \\beta_{K}(x_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Regression Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1. Piecewise Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Piecewise polynomial regression involves fitting separate low-degree polynomials over different regions of X\n",
    "$$ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{i}^{2} + \\beta_{3}x_{i}^{3} + \\epsilon_{i} $$\n",
    "\n",
    "where coefficients $\\beta_{0},\\beta_{1}, \\beta_{2}, \\beta_{3}$ different in different parts of the range of X. The point where the coefficents change are called $knots$\n",
    "\n",
    "$$ y_{i} = \\beta_{01} + \\beta_{11}x_{i} + \\beta_{21}x_{i}^{2} + \\beta_{31}x_{i}^{3} + \\epsilon_{i} \\space\\space\\text{ if } x_{i} < c \\\\ y_{i} = \\beta_{02} + \\beta_{12}x_{i} + \\beta_{22}x_{i}^{2} + \\beta_{32}x_{i}^{3} + \\epsilon_{i}  \\space\\space\\text{ if } x_{i} \\geq c$$\n",
    "\n",
    "- Using more knots leads to a more flexible piecewise polynomial\n",
    "    * If we place K different knots throughout the range of X, then we will end up fitting K+1 different cubic polynomials\n",
    "        + However, the function is dicontinuous and looks ridiculous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2. Constraints and Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to fit a piecewise polynomial under the constraint that the fitted curve must be continuous\n",
    "    * Adding of constraints help to ensure the piecewise polynomial is continuous and smooth\n",
    "        + Each constraint frees up one degree of freedom by reducing the complexity of the resulting piecewise polynomial fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3. The Spline Basis Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A basis model can be used to represent a regression spline\n",
    "    * Addition of one truncated power basis function per knot\n",
    "    * However, splines can have high variance at the outer range of the predictors\n",
    "        + Confidence bands in the boundary region can be fairly wild\n",
    "- A natural spline is a regression spline with additional contraints\n",
    "    * The function is required to be linear at the boundary \n",
    "        + This additional constraint means that natural splines generally produce more stable estimates at the boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.4. Choosing the Number and Locations of the Knots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression splines is most flexible in regions that contain a lot of knots as the polynomial coefficients can change rapidly\n",
    "    * Where to place the knots:\n",
    "        + One option is to place more knots in placess where we feel the function might vary the most rapidly\n",
    "        + In practice, knots are more commonly place in a uniform fashion\n",
    "    * How many knots should we use:\n",
    "        + One option is to try out different number of knots and see which produces the best looking curve\n",
    "        + A more objective approach is to use cross-validation where the value of K giving the smallest RSS is chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.5. Comparison to Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression splines often give superior results to polynomial regression\n",
    "    * Splines invludes the number of knots but keeping the degree fixed instead of increasing the degree like polynomial regression\n",
    "    * Splines allow us to place more knots and hence flexibility over regions where the function $f$ seems to be changing rapidly\n",
    "    * Extra flexibility in the polynomial produces undesirable results at the boundaries while the natural cubic splines still provides a reasonable fit to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Smoothing Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1. An Overview of Smoothing Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find a function $g(x)$ that fits the observed data well and minimizes RSS\n",
    "\n",
    "$$ \\sum^{n}_{i=1}(y_{i}-g(x_{i})^{2} + \\lambda \\int g''(t)^{2} dt $$\n",
    "where $\\lambda$ is a non-negative tuning parameter. $\\int g''(t)^{2}$ is a measure of the total change in the function $g'(t)$\n",
    "\n",
    "- If g is very smooth, then $g'(t)$ will be close to constant and $\\int g''(t)^{2}$ will take on a small value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2. Choosing the Smoothing Parameter λ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tuning parameter $\\lambda$ controls the roughness of the smoothing spline\n",
    "    * As $\\lambda$ increases from 0 to $\\infty$, the effective degrees of freedom $df_{\\lambda}$ decrease from n to 2\n",
    "    * $df_{\\lambda}$ is a measure of flexibility of the smoothing spline, the higher it is, the more flexible the smoothing spline\n",
    "        + Need to find the $\\lambda$ that makes the cross-validated RSS as small as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Local Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An approach for fitting flexible non-linear functions, which involves computing the fit at a target point $x_{0}$ using only the nearby training observations\n",
    "    * The weights will differ for each value of $x_{0}$\n",
    "- To perform local regression, there are a number of choices to be made\n",
    "    * Defining the weighting function K, whether to fit a linear, constant or quadratic regression\n",
    "    * The most important choice is the span $s$\n",
    "        + Controls the flexibility of the non-linear fit\n",
    "        + The smaller the value, the more local and wiggly the fit will be\n",
    "- Local regression generalizes very naturally when we want to fit models that are local in a pair of variables $X_{1}$ and $X_{2}$ rather than one\n",
    "    * Can use two-dimensional neighborhoods and fit bivaraite linear regression models using observations that are near each target point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Generalized Additive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generalized additive models (GAMs) provide a general framework for generalized additive model extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.\n",
    "    * Can be applied with both quantitative and qualitative responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.1 GAMs for Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_{i} = \\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + ... + + \\beta_{p}x_{ip} + \\epsilon_{i}$$\n",
    "\n",
    "- To allow for non-linear relationships between each feature and the response\n",
    "    * Replace each linear component $B_{j}x_{ij}$ with a (smooth) non-linear function $f_{j}(x_{i1}$\n",
    "        + Additive model because we calculate a separate $f_{j}$ for each $X_{j}$\n",
    "\n",
    "$$ y_{i} = \\beta_{0} + f_{1}(x_{i1} + + f_{2}x_{i2} + ... + f_{p}(x_{ip}) + \\epsilon_{i}$$\n",
    "\n",
    "**Pros and Cons of GAMs**\n",
    "- GAMs allows to fit a non-linear $f_{j}$ to each $X_{j}$\n",
    "- The non-linear fits can potentially make more accurate predictions for the response Y\n",
    "- Because the model is additive, we cans till examine the effect of each $X_{j}$ on Y individually while holding all the other variables fixed\n",
    "- Smoothness of the function $f_{j}$ for the variable $X_{j}$ can be summarized via degrees of freedom\n",
    "- The main limitation of GAMs is that the model is restricted to be additive.\n",
    "    * With many variables, important interactions can be missed\n",
    "        + However, we can manually add interaction terms to the GAM model by including additional predictors of the form $X_{j} \\times X_{k}$\n",
    "- GAMs can also be used for classification problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
