{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area of computer science and artifical intelligence concerned with the interactions between computers and human languages, in particular how to program computer to process and analyze large amounts of natural language data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP Basics**\n",
    "- [1. Tokenization](#1.-Tokenization)\n",
    "    * [1.1. Part-of-Speech Tagging (POS)](#1.1.-Part-of-Speech-Tagging-(POS))\n",
    "    * [1.2. Dependencies](#1.2.-Dependencies)\n",
    "    * [1.3. Named Entities](#1.3.-Named-Entities)\n",
    "    * [1.4. Noun Chunks](#1.4.-Noun-Chunks)\n",
    "    * [1.5. Additional Token Attributes](#1.5.-Additional-Token-Attributes)\n",
    "        + [1.5.1. Stemming/Lemmatization](#1.5.1.-Stemming/Lemmatization)\n",
    "        + [1.5.2. Stop Words](#1.5.2.-Stop-Words)\n",
    "        + [1.5.3. Spans](#1.5.3.-Spans)\n",
    "        + [1.5.4. Sentences](#1.5.4.-Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing of libraries\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Other libraries include: 'en_core_web_md' and 'en_core_web_lg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Process of breaking up the original text into components pieces (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla           PROPN           nsubj          \n",
      "is              AUX             aux            \n",
      "n't             PART            neg            \n",
      "looking         VERB            ROOT           \n",
      "into            ADP             prep           \n",
      "startups        NOUN            pobj           \n",
      "anymore         ADV             advmod         \n",
      ".               PUNCT           punct          \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Tesla isn't looking into startups anymore.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:{15}} {token.pos_:{15}} {token.dep_:{15}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Part-of-Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- POS tagging or grammatical tagging is the process of making up a word in a text as corresponding to a particular part of speech, based on its definition and its context\n",
    "    * For a full list of POS Tags visit: https://spacy.io/api/annotation#pos-tagging\n",
    "- To view the **coarse** POS tag use `token.pos_`\n",
    "- To view the **fine-grained** tag use `token.tag_`\n",
    "- To view the description of either type of tag use `spacy.explain(tag)`\n",
    "\n",
    "<div class=\"alert alert-success\">Note that `token.pos` and `token.tag` return integer hash values; by adding the underscores we get the text equivalent that lives in **doc.vocab**.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla           PROPN           NNP             noun, proper singular\n",
      "is              AUX             VBZ             verb, 3rd person singular present\n",
      "n't             PART            RB              adverb\n",
      "looking         VERB            VBG             verb, gerund or present participle\n",
      "into            ADP             IN              conjunction, subordinating or preposition\n",
      "startups        NOUN            NNS             noun, plural\n",
      "anymore         ADV             RB              adverb\n",
      ".               PUNCT           .               punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:{15}} {token.pos_:{15}} {token.tag_:{15}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 1, 87: 1, 94: 1, 100: 1, 85: 1, 92: 1, 86: 1, 97: 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting POS Tags\n",
    "\n",
    "POS_counts = doc.count_by(spacy.attrs.POS)\n",
    "POS_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between \"head\" words and words which modify those heads\n",
    "    * For a full list of Syntactic Dependencies visit https://spacy.io/api/annotation#dependency-parsing\n",
    "\n",
    "<div class=\"alert alert-success\">Note that `token.dep` return integer hash values; by adding the underscores we get the text equivalent that lives in **doc.vocab**.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla           nsubj          \n",
      "is              aux            \n",
      "n't             neg            \n",
      "looking         ROOT           \n",
      "into            prep           \n",
      "startups        pobj           \n",
      "anymore         advmod         \n",
      ".               punct          \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:{15}} {token.dep_:{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400. advmod: 1\n",
      "405. aux : 1\n",
      "425. neg : 1\n",
      "429. nsubj: 1\n",
      "439. pobj: 1\n",
      "443. prep: 1\n",
      "445. punct: 1\n",
      "8206900633647566924. ROOT: 1\n"
     ]
    }
   ],
   "source": [
    "# Count the different dependencies:\n",
    "DEP_counts = doc.count_by(spacy.attrs.DEP)\n",
    "\n",
    "for k,v in sorted(DEP_counts.items()):\n",
    "    print(f'{k}. {doc.vocab[k].text:{4}}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Named entity recognition (NER) is the task of identifying and categorizing key information (entities) in text\n",
    "    * An entity can be any word or series of words that consistently refers to the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to display basic entity info:\n",
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n",
    "    else:\n",
    "        print('No named entities found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington, DC - GPE - Countries, cities, states\n",
      "next May - DATE - Absolute or relative dates or periods\n",
      "the Washington Monument - ORG - Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')\n",
    "\n",
    "show_ents(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington, DC 4 7 12 26 GPE\n",
      "next May 7 9 27 35 DATE\n",
      "the Washington Monument 11 14 43 66 ORG\n"
     ]
    }
   ],
   "source": [
    "# Entity annotations\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Noun Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Doc.noun_chunks` are *base noun phrases*: token spans that include the noun and words describing the noun. \n",
    "- Noun chunks cannot be nested, cannot overlap, and do not involve prepositional phrases or relative clauses.<br>\n",
    "- Where `Doc.ents` rely on the **ner** pipeline component, `Doc.noun_chunks` are provided by the **parser**.\n",
    "    * For more on **noun_chunks** visit https://spacy.io/usage/linguistic-features#noun-chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars - cars - nsubj - shift\n",
      "insurance liability - liability - dobj - shift\n",
      "manufacturers - manufacturers - pobj - toward\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
    "\n",
    "for chunk in doc2.noun_chunks:\n",
    "    print(chunk.text+' - '+chunk.root.text+' - '+chunk.root.dep_+' - '+chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Additional Token Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1. Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stemming:**\n",
    "    * Crude method for cataloging related words; essentially chops off letter from the end until the stem is reached\n",
    "- **Lemmatization:**\n",
    "    * Looks beyond word reduction, and considers a language's full vocabulary to apply a morphological analysis to words\n",
    "        + Lemmatization is therefore better than stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          PRON         561228191312463089 -PRON-         \n",
      "am         AUX        10382539506755952630 be             \n",
      "a          DET        11901859001352538922 a              \n",
      "runner     NOUN       12640964157389618806 runner         \n",
      "running    VERB       12767647472892411841 run            \n",
      "in         ADP         3002984154512732771 in             \n",
      "a          DET        11901859001352538922 a              \n",
      "race       NOUN        8048469955494714898 race           \n",
      "because    SCONJ      16950148841647037698 because        \n",
      "I          PRON         561228191312463089 -PRON-         \n",
      "love       VERB        3702023516439754181 love           \n",
      "to         PART        3791531372978436496 to             \n",
      "run        VERB       12767647472892411841 run            \n",
      "since      SCONJ      10066841407251338481 since          \n",
      "I          PRON         561228191312463089 -PRON-         \n",
      "ran        VERB       12767647472892411841 run            \n",
      "today      NOUN       11042482332948150395 today          \n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "doc3 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc3:\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.lemma:{20}} {token.lemma_:{15}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2. Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Words like \"a\" and \"the\" appear so frequently that they don't require tagging as thoroughly as nouns, verbs and modifiers. We call these **stop words**, and they can be filtered from the text to be processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Checking if a word is a stop word\n",
    "\n",
    "print(nlp.vocab['myself'].is_stop)\n",
    "\n",
    "print(nlp.vocab['mystery'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the word to the set of stop words. Use lowercase!\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the word from the set of stop words\n",
    "nlp.Defaults.stop_words.remove('btw')\n",
    "\n",
    "# Remove the stop_word tag from the lexeme\n",
    "nlp.vocab['btw'].is_stop = False\n",
    "\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3. Spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **span** is a slice of the Doc object in the form `Doc[start:stop]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Life is what happens to us while we are making other plans\"\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')\n",
    "\n",
    "life_quote = doc4[16:30]\n",
    "print(life_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4. Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `sents` tag facilitates the segementation of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "doc5 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n",
    "\n",
    "for sent in doc5.sents:\n",
    "    print(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
