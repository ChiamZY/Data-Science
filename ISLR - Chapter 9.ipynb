{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 9\n",
    "\n",
    "- [9. Unsupervised Learning](#9.-Unsupervised-Learning)\n",
    "    * [9.1. The Challenge of Unsupervised Learning](#9.1.-The-Challenge-of-Unsupervised-Learning)\n",
    "    * [9.2. Principal Components Analysis](#9.2.-Principal-Components-Analysis)\n",
    "        + [9.2.1. What Are Principal Components?](#9.2.1.-What-Are-Principal-Components?)\n",
    "        + [9.2.2. Another Interpretation of Principal Components](#9.2.2.-Another-Interpretation-of-Principal-Components)\n",
    "        + [9.2.3. More on PCA](#9.2.3.-More-on-PCA)\n",
    "    * [9.3. Clustering Methods](#9.3.-Clustering-Methods)\n",
    "        + [9.3.1. K-Means Clustering](#9.3.1.-K-Means-Clustering)\n",
    "        + [9.3.2. Hierarchical Clustering](#9.3.2.-Hierarchical-Clustering)\n",
    "        + [9.3.3. Practical Issues in Clustering](#9.3.3.-Practical-Issues-in-Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A set of statistical tools intending for the setting in which we only have a set of features \n",
    "    * Not interested in prediction as we do not have an associated response variable Y\n",
    "- Two particular types of unsupervised learning:\n",
    "    * Principal component analysis\n",
    "    * Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. The Challenge of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unsupervised learning is often performed as part of an exploratory data analysis (EDA)\n",
    "- Compared to supervised learning, there is no way to assess the results obtained from unsupervised learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal components allow us to summarize the large set of correlated variables with a smaller number of representative variables that collectively explain most of the variability in the original set\n",
    "    * The principal component directions in the feature space along which the original data are high variable\n",
    "- Principal component Analysis (PCA):\n",
    "    * Process by which principal compoents are computed\n",
    "    * Subsequent use of these components in understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1. What Are Principal Components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA finds a low-dimensional representation of a data set that contains as much as possible of the the variation\n",
    "    * The ideas is that each of the $n$ observations lives in *p*-dimensional space but not all of these dimensions are equally interesting\n",
    "- The first prinicipal component of a set of features $X_{1}, X_{2}, ... , X_{p}$ is the normalized linear combination of the features that has the largest variance\n",
    "\n",
    "$$ Z_{1} = \\phi_{11}X_{1} + \\phi_{21}X_{2} + ... + \\phi_{p1}X_{p}$$\n",
    "\n",
    "where $\\phi_{11}, ... , \\phi_{p1}$ are loadings of the first principal component. By normalized, $\\sum^{p}_{j=1}\\phi^{2}_{j1}=1$\n",
    "\n",
    "**Computing the Principal Components**\n",
    "- For the first principal component:\n",
    "    * Each of the variables in X has to be centered with mean zero\n",
    "    * Look for the linear combination of the sample feature values that has the largest sample variance\n",
    "        + $z_{11}, ... , z_{n1}$ are the scores of the first principal component\n",
    "- For the second principal component:\n",
    "    * Linear combination of $X_{1},...,X_{p}$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_{1}$\n",
    "        + Constraining $Z_{2}$ to be uncorrelated with $Z_{1}$ is equivalent to constraining the direction $\\phi_{2}$ to be orthogonal (perpendicular) to the direction of $\\phi_{1}$\n",
    "        \n",
    "**Visualization of PCA**\n",
    "- Biplot:\n",
    "    * Displays both the principal component scores and the principal component loads in the figure\n",
    "- The loading vectors suggest which variables are more likely correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2. Another Interpretation of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal components provide low-dimensional linear surfaces that are closest to the observations\n",
    "    * The first principal component loading vector is the line in p-dimensional space that is closest to the n observations\n",
    "        + Measured using averaged squared Euclidean distance as a measure of closeness\n",
    "    * The first M principal components of a data set span the three-dimensional hyperplane that is closest to the n observations\n",
    "        + Together, the M principal component scores vectors and M principal component loading vectors can give a good approximation to the data when M is sufficiently large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3. More on PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling the Variables**\n",
    "- Variables should be centered to have mean zero\n",
    "    * Results obtained when we perform PCA will also depend on whether the variables have been individually scaled\n",
    "- If PCA is performed on unscaled variables\n",
    "    * The first principal component will have very large loading vectors for variables with very large units\n",
    "- For instances where the variables are measured in the same units, scaling the variables is not necessary\n",
    "\n",
    "**Uniqueness of the Principal Components**\n",
    "- Each principal component loading vector is unique up to a sign flip\n",
    "    * Two different software packages will yield the same principal component loading vectors, although the signs of those loading vectors may differ\n",
    "        + Flipping the sign has no effect as the direction does not change\n",
    "        \n",
    "**The Proportion of Variance Explained**\n",
    "- How much of the information in a given data set is lost by projecting the observations onto the first few principal components\n",
    "    * The proportion of variance explained (PVE) of each principal component is a positive quantity\n",
    "        + In order to compute the cumulative PVE of the first M principal components, we can sum over each of the first M PVEs.\n",
    "\n",
    "**Deciding How Many Principal Components to Use**\n",
    "- Use the smallest number of principal components required to get a good understanding of the data\n",
    "    * A scree plot is often used to determine the point at which the proportion of variance explained by each subsequent principal component drops off (elbow method)\n",
    "- Ultimately there is no well-accepted objective way to decide how many principal components are enough\n",
    "\n",
    "**Other Uses for Principal Components**\n",
    "- Perform regression using the principal component score vectors as features\n",
    "    * Less noisy results as the data set is concentrated in its first few principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering refers to a very broad set of techniques for finding subgroups in a dataset\n",
    "    * Partition the observations into distinct groups so that the observations within each group are quite similar from each other\n",
    "\n",
    "**Difference between PCA and Clustering**\n",
    "- PCA:\n",
    "    * Find a low-dimnesional representation of the observations that explain a good fraction of the variance\n",
    "- Clustering:\n",
    "    * Find homogeneous subgroups among the observations\n",
    "\n",
    "**Types of Clustering**\n",
    "- K-means clustering:\n",
    "    * Partition the observations into a pre-specified number of clusters\n",
    "- Hierarchical clustering\n",
    "    * Do not know how many clusters we want, end up with a tree-like visual representation of the observations, called a dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters\n",
    "    * Specify the desired number of clusters K\n",
    "    * Assign each observation to exactly on of the K clusters (non-overlapping)\n",
    "- K-means cluster trys to minimize the within-cluster variation to as small as possible\n",
    "    * Most common choice involves squared Euclidean distance\n",
    "        + The within-cluster variation for the kth cluster is the sum of all the pairwise squared Euclidean distances between the observations in the kth cluster divided by the total number of observations in the kth cluster\n",
    "\n",
    "**K-Means Clustering Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3. Practical Issues in Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
