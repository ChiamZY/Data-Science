{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 9\n",
    "\n",
    "- [9. Unsupervised Learning](#9.-Unsupervised-Learning)\n",
    "    * [9.1. The Challenge of Unsupervised Learning](#9.1.-The-Challenge-of-Unsupervised-Learning)\n",
    "    * [9.2. Principal Components Analysis](#9.2.-Principal-Components-Analysis)\n",
    "        + [9.2.1. What Are Principal Components?](#9.2.1.-What-Are-Principal-Components?)\n",
    "        + [9.2.2. Another Interpretation of Principal Components](#9.2.2.-Another-Interpretation-of-Principal-Components)\n",
    "        + [9.2.3. More on PCA](#9.2.3.-More-on-PCA)\n",
    "    * [9.3. Clustering Methods](#9.3.-Clustering-Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A set of statistical tools intending for the setting in which we only have a set of features \n",
    "    * Not interested in prediction as we do not have an associated response variable Y\n",
    "- Two particular types of unsupervised learning:\n",
    "    * Principal component analysis\n",
    "    * Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. The Challenge of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unsupervised learning is often performed as part of an exploratory data analysis (EDA)\n",
    "- Compared to supervised learning, there is no way to assess the results obtained from unsupervised learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal components allow us to summarize the large set of correlated variables with a smaller number of representative variables that collectively explain most of the variability in the original set\n",
    "    * The principal component directions in the feature space along which the original data are high variable\n",
    "- Principal component Analysis (PCA):\n",
    "    * Process by which principal compoents are computed\n",
    "    * Subsequent use of these components in understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1. What Are Principal Components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA finds a low-dimensional representation of a data set that contains as much as possible of the the variation\n",
    "    * The ideas is that each of the $n$ observations lives in *p*-dimensional space but not all of these dimensions are equally interesting\n",
    "- The first prinicipal component of a set of features $X_{1}, X_{2}, ... , X_{p}$ is the normalized linear combination of the features that has the largest variance\n",
    "\n",
    "$$ Z_{1} = \\phi_{11}X_{1} + \\phi_{21}X_{2} + ... + \\phi_{p1}X_{p}$$\n",
    "\n",
    "where $\\phi_{11}, ... , \\phi_{p1}$ are loadings of the first principal component. By normalized, $\\sum^{p}_{j=1}\\phi^{2}_{j1}=1$\n",
    "\n",
    "**Computing the Principal Components**\n",
    "- For the first principal component:\n",
    "    * Each of the variables in X has to be centered with mean zero\n",
    "    * Look for the linear combination of the sample feature values that has the largest sample variance\n",
    "        + $z_{11}, ... , z_{n1}$ are the scores of the first principal component\n",
    "- For the second principal component:\n",
    "    * Linear combination of $X_{1},...,X_{p}$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_{1}$\n",
    "        + Constraining $Z_{2}$ to be uncorrelated with $Z_{1}$ is equivalent to constraining the direction $\\phi_{2}$ to be orthogonal (perpendicular) to the direction of $\\phi_{1}$\n",
    "        \n",
    "**Visualization of PCA**\n",
    "- Biplot:\n",
    "    * Displays both the principal component scores and the principal component loads in the figure\n",
    "- The loading vectors suggest which variables are more likely correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2. Another Interpretation of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal components provide low-dimensional linear surfaces that are closest to the observations\n",
    "    * The first principal component loading vector is the line in p-dimensional space that is closest to the n observations\n",
    "        + Measured using averaged squared Euclidean distance as a measure of closeness\n",
    "    * The first M principal components of a data set span the three-dimensional hyperplane that is closest to the n observations\n",
    "        + Together, the M principal component scores vectors and M principal component loading vectors can give a good approximation to the data when M is sufficiently large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3. More on PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling the Variables**\n",
    "- Variables should be centered to have mean zero\n",
    "    * Results obtained when we perform PCA will also depend on whether the variables have been individually scaled\n",
    "- If PCA is performed on unscaled variables\n",
    "    * The first principal component will have very large loading vectors for variables with very large units\n",
    "- For instances where the variables are measured in the same units, scaling the variables is not necessary\n",
    "\n",
    "**Uniqueness of the Principal Components**\n",
    "- Each principal component loading vector is unique up to a sign flip\n",
    "    * Two different software packages will yield the same principal component loading vectors, although the signs of those loading vectors may differ\n",
    "        + Flipping the sign has no effect as the direction does not change\n",
    "        \n",
    "**The Proportion of Variance Explained**\n",
    "- How much of the information in a given data set is lost by projecting the observations onto the first few principal components\n",
    "    * The proportion of variance explained (PVE) of each principal component is a positive quantity\n",
    "        + In order to compute the cumulative PVE of the first M principal components, we can sum over each of the first M PVEs.\n",
    "\n",
    "**Deciding How Many Principal Components to Use**\n",
    "- Use the smallest number of principal components required to get a good understanding of the data\n",
    "    * A scree plot is often used to determine the point at which the proportion of variance explained by each subsequent principal component drops off (elbow method)\n",
    "- Ultimately there is no well-accepted objective way to decide how many principal components are enough\n",
    "\n",
    "**Other Uses for Principal Components**\n",
    "- Perform regression using the principal component score vectors as features\n",
    "    * Less noisy results as the data set is concentrated in its first few principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
