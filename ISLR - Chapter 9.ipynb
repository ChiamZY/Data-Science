{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 9\n",
    "\n",
    "- [9. Unsupervised Learning](#9.-Unsupervised-Learning)\n",
    "    * [9.1. The Challenge of Unsupervised Learning](#9.1.-The-Challenge-of-Unsupervised-Learning)\n",
    "    * [9.2. Principal Components Analysis](#9.2.-Principal-Components-Analysis)\n",
    "        + [9.2.1. What Are Principal Components?](#9.2.1.-What-Are-Principal-Components?)\n",
    "        + [9.2.2. Another Interpretation of Principal Components](#9.2.2.-Another-Interpretation-of-Principal-Components)\n",
    "        + [9.2.3. More on PCA](#9.2.3.-More-on-PCA)\n",
    "    * [9.3. Clustering Methods](#9.3.-Clustering-Methods)\n",
    "        + [9.3.1. K-Means Clustering](#9.3.1.-K-Means-Clustering)\n",
    "        + [9.3.2. Hierarchical Clustering](#9.3.2.-Hierarchical-Clustering)\n",
    "        + [9.3.3. Practical Issues in Clustering](#9.3.3.-Practical-Issues-in-Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A set of statistical tools intending for the setting in which we only have a set of features \n",
    "    * Not interested in prediction as we do not have an associated response variable Y\n",
    "- Two particular types of unsupervised learning:\n",
    "    * Principal component analysis\n",
    "    * Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. The Challenge of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unsupervised learning is often performed as part of an exploratory data analysis (EDA)\n",
    "- Compared to supervised learning, there is no way to assess the results obtained from unsupervised learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal components allow us to summarize the large set of correlated variables with a smaller number of representative variables that collectively explain most of the variability in the original set\n",
    "    * The principal component directions in the feature space along which the original data are high variable\n",
    "- Principal component Analysis (PCA):\n",
    "    * Process by which principal compoents are computed\n",
    "    * Subsequent use of these components in understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1. What Are Principal Components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA finds a low-dimensional representation of a data set that contains as much as possible of the the variation\n",
    "    * The ideas is that each of the $n$ observations lives in *p*-dimensional space but not all of these dimensions are equally interesting\n",
    "- The first prinicipal component of a set of features $X_{1}, X_{2}, ... , X_{p}$ is the normalized linear combination of the features that has the largest variance\n",
    "\n",
    "$$ Z_{1} = \\phi_{11}X_{1} + \\phi_{21}X_{2} + ... + \\phi_{p1}X_{p}$$\n",
    "\n",
    "where $\\phi_{11}, ... , \\phi_{p1}$ are loadings of the first principal component. By normalized, $\\sum^{p}_{j=1}\\phi^{2}_{j1}=1$\n",
    "\n",
    "**Computing the Principal Components**\n",
    "- For the first principal component:\n",
    "    * Each of the variables in X has to be centered with mean zero\n",
    "    * Look for the linear combination of the sample feature values that has the largest sample variance\n",
    "        + $z_{11}, ... , z_{n1}$ are the scores of the first principal component\n",
    "- For the second principal component:\n",
    "    * Linear combination of $X_{1},...,X_{p}$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_{1}$\n",
    "        + Constraining $Z_{2}$ to be uncorrelated with $Z_{1}$ is equivalent to constraining the direction $\\phi_{2}$ to be orthogonal (perpendicular) to the direction of $\\phi_{1}$\n",
    "        \n",
    "**Visualization of PCA**\n",
    "- Biplot:\n",
    "    * Displays both the principal component scores and the principal component loads in the figure\n",
    "- The loading vectors suggest which variables are more likely correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2. Another Interpretation of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal components provide low-dimensional linear surfaces that are closest to the observations\n",
    "    * The first principal component loading vector is the line in p-dimensional space that is closest to the n observations\n",
    "        + Measured using averaged squared Euclidean distance as a measure of closeness\n",
    "    * The first M principal components of a data set span the three-dimensional hyperplane that is closest to the n observations\n",
    "        + Together, the M principal component scores vectors and M principal component loading vectors can give a good approximation to the data when M is sufficiently large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3. More on PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling the Variables**\n",
    "- Variables should be centered to have mean zero\n",
    "    * Results obtained when we perform PCA will also depend on whether the variables have been individually scaled\n",
    "- If PCA is performed on unscaled variables\n",
    "    * The first principal component will have very large loading vectors for variables with very large units\n",
    "- For instances where the variables are measured in the same units, scaling the variables is not necessary\n",
    "\n",
    "**Uniqueness of the Principal Components**\n",
    "- Each principal component loading vector is unique up to a sign flip\n",
    "    * Two different software packages will yield the same principal component loading vectors, although the signs of those loading vectors may differ\n",
    "        + Flipping the sign has no effect as the direction does not change\n",
    "        \n",
    "**The Proportion of Variance Explained**\n",
    "- How much of the information in a given data set is lost by projecting the observations onto the first few principal components\n",
    "    * The proportion of variance explained (PVE) of each principal component is a positive quantity\n",
    "        + In order to compute the cumulative PVE of the first M principal components, we can sum over each of the first M PVEs.\n",
    "\n",
    "**Deciding How Many Principal Components to Use**\n",
    "- Use the smallest number of principal components required to get a good understanding of the data\n",
    "    * A scree plot is often used to determine the point at which the proportion of variance explained by each subsequent principal component drops off (elbow method)\n",
    "- Ultimately there is no well-accepted objective way to decide how many principal components are enough\n",
    "\n",
    "**Other Uses for Principal Components**\n",
    "- Perform regression using the principal component score vectors as features\n",
    "    * Less noisy results as the data set is concentrated in its first few principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering refers to a very broad set of techniques for finding subgroups in a dataset\n",
    "    * Partition the observations into distinct groups so that the observations within each group are quite similar from each other\n",
    "\n",
    "**Difference between PCA and Clustering**\n",
    "- PCA:\n",
    "    * Find a low-dimnesional representation of the observations that explain a good fraction of the variance\n",
    "- Clustering:\n",
    "    * Find homogeneous subgroups among the observations\n",
    "\n",
    "**Types of Clustering**\n",
    "- K-means clustering:\n",
    "    * Partition the observations into a pre-specified number of clusters\n",
    "- Hierarchical clustering\n",
    "    * Do not know how many clusters we want, end up with a tree-like visual representation of the observations, called a dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters\n",
    "    * Specify the desired number of clusters K\n",
    "    * Assign each observation to exactly on of the K clusters (non-overlapping)\n",
    "- K-means cluster trys to minimize the within-cluster variation to as small as possible\n",
    "    * Most common choice involves squared Euclidean distance\n",
    "        + The within-cluster variation for the kth cluster is the sum of all the pairwise squared Euclidean distances between the observations in the kth cluster divided by the total number of observations in the kth cluster\n",
    "\n",
    "**K-Means Clustering Algorithm**\n",
    "- Randomly assign a number from 1 to K to each of the observations\n",
    "- Iterate until the cluster assignment stop changing:\n",
    "    * For each of the K clusters, compute the cluster centroid\n",
    "        + The kth cluster centroid is the vector of the p feature means for the observations in the kth cluster\n",
    "    * Assign each observation ot the cluster whose centroid is the closest\n",
    "- Because K-means algorithm finds a local rather than a global optimum, the results obtain will depend on the initial (random) cluster assignment of each observation\n",
    "    * Therefore it is important to run the algorithm multiple times fom different random initial configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An alternative approach that does not require that we commit to a particular choice of K\n",
    "- Added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram\n",
    "    * Dendrogram is a bottom-up or agglomerative clustering\n",
    "    \n",
    "**Interpreting a Dendrogram**\n",
    "- Parts of the Dendrogram:\n",
    "    * Leaf: Represent each of the observation\n",
    "    * Branches: Similar observations (leaves) fuse into branches\n",
    "        + Branches can also fuse with other branches and leaves\n",
    "- The lower the fusion occur, the more similar the groups of observations are to each other\n",
    "    * The height of the fusion as measured on the vertical axis indicates how different the two observations are\n",
    "- There are $2^{n-1}$ possible reorderings of the dendrogram where n is the number of leaves\n",
    "    * We can only draw conclusions about the similarity of two observations based on the location of the vertical axis and not the horizontal axis\n",
    "- To identify clusters of a dendrogram, we can make a horizontal cut across the dendrogram\n",
    "    * The height of the cut to the dendrogram serves the same role as the K in K-means clustering as it controls the number of cluster obtained\n",
    "    \n",
    "**The Hierarchical Clustering Algorithm**\n",
    "- Define the dissimilarity measure between each pair of observations\n",
    "    * Most often, Euclidean distance is used\n",
    "- The first 2 clusters that are most similar to each other are fused so that there are now n-1 cluster\n",
    "    * Next the two cluster that most similar will fuse again so that there are now n-2 clusters\n",
    "        + Proceed till all of the observations belong to one single cluster\n",
    "- The concept of dissimilarity between a pair of observations can be extended to a pair of groups of observations. This extension is acheived by developing the notion of linkage\n",
    "    * Complete\n",
    "        + Maximal intercluster dissimilarity. \n",
    "        + Compute all pariwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities\n",
    "    * Average\n",
    "        + Mean intercluster dissimilarity. Compute all pariwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities\n",
    "    * Single\n",
    "        + Minimal intercluster dissimilarity.\n",
    "        + Compute all pariwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities\n",
    "        + Single linkage can result in extended, trailing cluster in which single observations are fused one at a time\n",
    "    * Centroid\n",
    "        + Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B\n",
    "        + Centroid linkage can result in undesirable inversions\n",
    "\n",
    "**Choice of Dissimilarity Measure**\n",
    "- Correlation-based distances considers two observations to be similar if their features are highly correlated even thou the observed values may be far apart in terms of Euclidean distance\n",
    "    * Focuses on the shapes of observations profile rather than their magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3. Practical Issues in Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Small Decisions with Big Consequences**\n",
    "- To perform clustering, some decisions must be made\n",
    "    * Should the observations or features first be standardized in some way?\n",
    "        + Variables may need to be centered to have mean zero and scaled to have standard deviation one\n",
    "    * In the case of hierarchical clustering\n",
    "        + What dissimilarity measure should be used?\n",
    "        + What type of linkage should be used?\n",
    "        + Where should we cut the dendrogram in order to obtain clusters\n",
    "    * In the case of K-means cluster, how many clusters should we look for in the data\n",
    "\n",
    "**Validating the Clusters Obtained**\n",
    "- Any time clustering is performed on a data set, we will find clusters\n",
    "    * But we want to know whether the clusters have been found representing true subgroups in the data or whether they are simply a result of clustering the noise\n",
    "\n",
    "**Other Considerations in Clustering**\n",
    "- Both K-means and hierarchical clustering will assign each observation to a cluster\n",
    "    * However sometimes this might not be appropraite\n",
    "        + This may result in the clusters to be heavily distorted due to the presence of outliers\n",
    "        \n",
    "**A Tempered Approach to Interpreting the Results of Clustering**\n",
    "- As small decisions in clustering, such as how the data is standardized and what type of linage is used can have a large effect on the results\n",
    "    * Performing clustering with different choices of parameters and looking at the full set of results to see which patterns consistently emerge can help\n",
    "        + These results can then constitute a starting point for the development of a scientific hypothesis and further study, preferably on an independent data set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
