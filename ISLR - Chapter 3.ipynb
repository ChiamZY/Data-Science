{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 3\n",
    "\n",
    "- [3. Classification](#3.-Classification)\n",
    "    * [3.1. Logistic Regression](#3.1.-Logistic-Regression)\n",
    "        + [3.1.1. The Logistic Model](#3.1.1.-The-Logistic-Model)\n",
    "        + [3.1.2. Estimating the Regression Coefficients](#3.1.2.-Estimating-the-Regression-Coefficients)\n",
    "        + [3.1.3. Making Predictions](#3.1.3.-Making-Predictions)\n",
    "        + [3.1.4. Multiple Logistic Regression](#3.1.4.-Multiple-Logistic-Regression)\n",
    "    * [3.2. Linear Discriminant Analysis](#3.2.-Linear-Discriminant-Analysis)\n",
    "        + [3.2.1. Using Bayes' Theorem for Classification](#3.2.1.-Using-Bayes'-Theorem-for-Classification)\n",
    "        + [3.2.2. Linear Discriminant Analysis (LDA) for p = 1](#3.2.2.-Linear-Discriminant-Analysis-(LDA)-for-p-=-1)\n",
    "        + [3.2.3 Linear Discriminant Analysis for p >1](#3.2.3-Linear-Discriminant-Analysis-for-p->1)\n",
    "        + [3.2.4 Quadratic Discriminant Analysis (QDA)](#3.2.4-Quadratic-Discriminant-Analysis-(QDA))\n",
    "    * [3.3. Comparison of Classification Methods](#3.3.-Comparison-of-Classification-Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A situation where the response variable is `qualitative` instead of quantitative\n",
    "    * Techniques used to predict a qualitative response include:\n",
    "        + Logistic regression\n",
    "        + Linear discriminant analysis\n",
    "        + K-nearest neighbours\n",
    "    * Other more computer-intensive methods include:\n",
    "        + Generalized additive models\n",
    "        + Trees and random forests and boosting\n",
    "        + Support vector machines\n",
    "- Forcing qualitative variables into a regression model wrongly assumes that the difference between the predictors is similar\n",
    "- Two types of qualitative data:\n",
    "    * Ordinal\n",
    "        + Variables with a specific order (Low, Medium, High)\n",
    "    * Nominal\n",
    "        + Variables with no specific order (Book, Television, Car)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logisitic regression models the probability that the response $Y$ belongs to a particular category\n",
    "    * Values will range between 0 and 1\n",
    "    \n",
    "$$ p(X) = Pr(Y = 1|X) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. The Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To model the $p(X)$ that gives outputs between 0 and 1 for all values of X, we can use the logistic function\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_{0} + \\beta_{1}X}}{1 + e^{\\beta_{0} + \\beta_{1}X}} $$\n",
    "\n",
    "where $\\frac{p(X)}{1-p(X)}$ is the `odds` and can take on any value between 0 and $\\infty$\n",
    "\n",
    "- Values of the odds close to 0 and $\\infty$ indicate *very low* and *high* probabilities of the response belonging to a particular category respectively\n",
    "- By taking the logarithm of both sides, we can get the log-odds or logit\n",
    "\n",
    "$$ log(\\frac{p(X)}{1-p(X)}) = \\beta_{0} + \\beta_{1}X $$\n",
    "\n",
    "where increasing $X$ by one unit changes the log odds by $\\beta_{1}$.\n",
    "- However, $\\beta_{1}$ does not correspond to the change in $p(X)$ associated with a one-unit increase in X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maximum likelihood to fit a logistic regression model\n",
    "    * Seeks estimates for $\\beta_{0}$ and $\\beta_{1}$ such that the predicted proability $\\hat{p}(X_{i})$ will correspond as closely as possible to the observed response.\n",
    "        + One unit increase in the predictor $X_{j}$ is associated with an increase in the log odds of the response (Y=1) by $\\hat{\\beta_{1}}$\n",
    "- To measure the accuracy of the coefficient estimates, the standard error can be computed\n",
    "    * *Z-statistic* is associated with $\\beta_{1}$ is equal to $\\hat{\\beta_{1}}/SE(\\hat{\\beta_{1}})$\n",
    "        + A large value of *z-statistic* indicates evidence against the null hypothesis (p < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Qualitative predictors can be used with the logistic regression model using the dummy variable approach\n",
    "    * We can assign a dummy variable that takes on a value of 1 and 0\n",
    "\n",
    "**For value of 1:**\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_{0} + \\beta_{1}X}}{1 + e^{\\beta_{0} + \\beta_{1}X}} $$\n",
    "\n",
    "**For value of 0:**\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_{0}}}{1 + e^{\\beta_{0}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log(\\frac{p(X)}{1-p(X)}) = \\beta_{0} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p} $$\n",
    "\n",
    "where $X = (X_{1}, ... , X_{p})$ are $p$ predictors. Can also be rewritten as\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_{0} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p}}}{1 + e^{\\beta_{0} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p}}} $$\n",
    "\n",
    "- As variables can be correlated, performing regression involving a single predictor could result in drastically different interpretations\n",
    "    * Confounding variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model the distribution of the predictors X separately in each of the response classes (**Y**)\n",
    "    * Use Bayes' theorem to flip these around into estimates for $Pr(Y=k|X=x)$\n",
    "- Benefits of LDA:\n",
    "    * When classes are well-separated, parameter estimates for logistic regression model are surprisely unstable\n",
    "        + LDA does not suffer from this problem\n",
    "    * When n is small and the distribution of the predictors X is approximately normal in each of the classes, LDA is more stable than logistic regression\n",
    "- LDA is better for >2 response classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Using Bayes' Theorem for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(Y=k|X=x) = \\frac{\\pi _{k}f_{k}(x)}{\\sum^{K}_{l=1}\\pi _{1}f_{1}(x)} $$\n",
    "\n",
    "where \n",
    "$\\pi_{k}$ represent the overall or prior probability that a randomly chosen observation comes from the kth class;  \n",
    "$f_{k}(x) = Pr(X=x|Y=k)$ denote the density function of X for an observation that comes from the kth class;  \n",
    "$p_{k}(X) = Pr(Y= k|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Linear Discriminant Analysis (LDA) for p = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f_{k}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{k}}exp\\left(-\\frac{1}{2\\sigma_{k}^{2}}(x-\\mu_{k})^{2}\\right)$$\n",
    "\n",
    "where $\\mu_{k}$ and $\\sigma_{k}^{2}$ are the mean and variance parameters for the kth class\n",
    "\n",
    "Assuming shared variance term across all **K** class, the posterior probability can be calculated as\n",
    "\n",
    "$$ p_{k}(x) = \\dfrac{\\pi_{k}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{1}{2\\sigma^{2}}(x-\\mu_{k})^{2}\\right)}{\\sum^{K}_{l=1}\\pi_{l}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{1}{2\\sigma^{2}}(x-\\mu_{l})^{2}\\right)} $$ \n",
    "\n",
    "- LDA classifier assumes that the observations within each class comes from a normal distribution with a class-specific mean vector and a common variance $\\sigma^{2}$, and pluggin estimates for these parameters into the Baye's classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Linear Discriminant Analysis for p >1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume that X is drawn from a multivariate Gaussian distribution, with a class-specific mean vector and a common covariance matrix\n",
    "- To indicate a $p$-dimensional random variable X has a multivariate Gaussian ditribution\n",
    "    * $X~N(\\mu,\\sum)$\n",
    "        + $E(X) = \\mu$ is the mean of **X** (A vector with p components)\n",
    "        + $Cov(X) = \\sum$ is the $p \\times p$ covariance of **X**\n",
    "$$ f(x) = \\frac{1}{(2\\pi)^{p/2}|\\sum|^{1/2}}exp\\left(-\\frac{1}{2}(x-\\mu)^{T}\\sum^{-1}(x-\\mu)\\right) $$\n",
    "- Bayes decision boundary then divides the predictors into the different number of regions\n",
    "- To determine the types of error made in a classification problem, numerous matrixs can be evaluated\n",
    "    * Confusion matrix \n",
    "    * Sensitivity\n",
    "    * Specificity\n",
    "\n",
    "**Why LDA might not be as good**\n",
    "- LDA tries to approximate the Bayes' classifier\n",
    "    * Lowest total error rate out of all the classifiers (if the Gaussian model is correct)\n",
    "- LDA does not differentiate where the class the errors come from\n",
    "    * Low sensitivity\n",
    "        + Can lower the threshold but results in a tradeoff, higher error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quadratic Discriminant Analysis is an alternative to LDA\n",
    "- Assumes each class has its own covariance matrix where $\\sum_{k}$ is a covariance matrix for the kth class\n",
    "    * Bayes classifier assigns an observation where the probability is the largest\n",
    "\n",
    "**Bias-variance trade-off between LDA vs QDA**\n",
    "- QDA:\n",
    "    * By estimating a different covariance matrix, we are estimating $p(p+1)/2$ parameters\n",
    "    * Better than LDA when the model is non-linear\n",
    "- LDA:\n",
    "    * Assuming a common covariance matrix, we are assuming the model to be linear\n",
    "    * LDA is much less flexible classifier than QDA and has substantially lower variance.\n",
    "    * Better when there are few training observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Comparison of Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA and Logistic Regression**\n",
    "- LDA and Logistic Regression are both linear functions of x.\n",
    "- LDA estimates the mean and variance from a normal distribution while Logistic Regression uses maximum likelihood\n",
    "- The assumption of a Gaussian distribution is the key in determining which method will outperform\n",
    "\n",
    "**QDA and KNN**\n",
    "- QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approach\n",
    "    * Assumes a quadratic decision boundary\n",
    "        + Good when the model is moderately non0linear\n",
    "- KNN is superior when the model is very non-linear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
