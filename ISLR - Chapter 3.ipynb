{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 3\n",
    "\n",
    "- [3. Classification](#3.-Classification)\n",
    "    * [3.1. Logistic Regression](#3.1.-Logistic-Regression)\n",
    "        + [3.1.1. The Logistic Model](#3.1.1.-The-Logistic-Model)\n",
    "        + [3.1.2. Estimating the Regression Coefficients](#3.1.2.-Estimating-the-Regression-Coefficients)\n",
    "        + [3.1.3. Making Predictions](#3.1.3.-Making-Predictions)\n",
    "        + [3.1.4. Multiple Logistic Regression](#3.1.4.-Multiple-Logistic-Regression)\n",
    "    * [3.2. Linear Discriminant Analysis](#3.2.-Linear-Discriminant-Analysis)\n",
    "        + [3.2.1. Using Bayes' Theorem for Classification](#3.2.1.-Using-Bayes'-Theorem-for-Classification)\n",
    "        + [3.2.2. Linear Discriminant Analysis for p = 1](#3.2.2.-Linear-Discriminant-Analysis-for-p-=-1)\n",
    "        + [3.2.3 Linear Discriminant Analysis for p >1](#3.2.3-Linear-Discriminant-Analysis-for-p->1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A situation where the response variable is `qualitative` instead of quantitative\n",
    "    * Techniques used to predict a qualitative response include:\n",
    "        + Logistic regression\n",
    "        + Linear discriminant analysis\n",
    "        + K-nearest neighbours\n",
    "    * Other more computer-intensive methods include:\n",
    "        + Generalized additive models\n",
    "        + Trees and random forests and boosting\n",
    "        + Support vector machines\n",
    "- Forcing qualitative variables into a regression model wrongly assumes that the difference between the predictors is similar\n",
    "- Two types of qualitative data:\n",
    "    * Ordinal\n",
    "        + Variables with a specific order (Low, Medium, High)\n",
    "    * Nominal\n",
    "        + Variables with no specific order (Book, Television, Car)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logisitic regression models the probability that the response $Y$ belongs to a particular category\n",
    "    * Values will range between 0 and 1\n",
    "    \n",
    "$$ p(X) = Pr(Y = 1|X) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. The Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To model the $p(X)$ that gives outputs between 0 and 1 for all values of X, we can use the logistic function\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_{0} + \\beta_{1}X}}{1 + e^{\\beta_{0} + \\beta_{1}X}} $$\n",
    "\n",
    "where $\\frac{p(X)}{1-p(X)}$ is the `odds` and can take on any value between 0 and $\\infty$\n",
    "\n",
    "- Values of the odds close to 0 and $\\infty$ indicate *very low* and *high* probabilities of the response belonging to a particular category respectively\n",
    "- By taking the logarithm of both sides, we can get the log-odds or logit\n",
    "\n",
    "$$ log(\\frac{p(X)}{1-p(X)}) = \\beta_{0} + \\beta_{1}X $$\n",
    "\n",
    "where increasing $X$ by one unit changes the log odds by $\\beta_{1}$.\n",
    "- However, $\\beta_{1}$ does not correspond to the change in $p(X)$ associated with a one-unit increase in X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maximum likelihood to fit a logistic regression model\n",
    "    * Seeks estimates for $\\beta_{0}$ and $\\beta_{1}$ such that the predicted proability $\\hat{p}(X_{i})$ will correspond as closely as possible to the observed response.\n",
    "        + One unit increase in the predictor $X_{j}$ is associated with an increase in the log odds of the response (Y=1) by $\\hat{\\beta_{1}}$\n",
    "- To measure the accuracy of the coefficient estimates, the standard error can be computed\n",
    "    * *Z-statistic* is associated with $\\beta_{1}$ is equal to $\\hat{\\beta_{1}}/SE(\\hat{\\beta_{1}})$\n",
    "        + A large value of *z-statistic* indicates evidence against the null hypothesis (p < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Qualitative predictors can be used with the logistic regression model using the dummy variable approach\n",
    "    * We can assign a dummy variable that takes on a value of 1 and 0\n",
    "\n",
    "**For value of 1:**\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_{0} + \\beta_{1}X}}{1 + e^{\\beta_{0} + \\beta_{1}X}} $$\n",
    "\n",
    "**For value of 0:**\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_{0}}}{1 + e^{\\beta_{0}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log(\\frac{p(X)}{1-p(X)}) = \\beta_{0} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p} $$\n",
    "\n",
    "where $X = (X_{1}, ... , X_{p})$ are $p$ predictors. Can also be rewritten as\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_{0} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p}}}{1 + e^{\\beta_{0} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p}}} $$\n",
    "\n",
    "- As variables can be correlated, performing regression involving a single predictor could result in drastically different interpretations\n",
    "    * Confounding variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model the distribution of the predictors X separately in each of the response classes (**Y**)\n",
    "    * Use Bayes' theorem to flip these around into estimates for $Pr(Y=k|X=x)$\n",
    "- Benefits of LDA:\n",
    "    * When classes are well-separated, parameter estimates for logistic regression model are surprisely unstable\n",
    "        + LDA does not suffer from this problem\n",
    "    * When n is small and the distribution of the predictors X is approximately normal in each of the classes, LDA is more stable than logistic regression\n",
    "- LDA is better for >2 response classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Using Bayes' Theorem for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(Y=k|X=x) = \\frac{\\pi _{k}f_{k}(x)}{\\sum^{K}_{l=1}\\pi _{1}f_{1}(x)} $$\n",
    "\n",
    "where \n",
    "$\\pi_{k}$ represent the overall or prior probability that a randomly chosen observation comes from the kth class;  \n",
    "$f_{k}(x) = Pr(X=x|Y=k)$ denote the density function of X for an observation that comes from the kth class;  \n",
    "$p_{k}(X) = Pr(Y= k|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Linear Discriminant Analysis for p = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f_{k}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{k}}exp\\left(-\\frac{1}{2\\sigma_{k}^{2}}(x-\\mu_{k})^{2}\\right)$$\n",
    "\n",
    "where $\\mu_{k}$ and $\\sigma_{k}^{2}$ are the mean and variance parameters for the kth class\n",
    "\n",
    "Assuming shared variance term across all **K** class, the posterior probability can be calculated as\n",
    "\n",
    "$$ p_{k}(x) = \\dfrac{\\pi_{k}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{1}{2\\sigma^{2}}(x-\\mu_{k})^{2}\\right)}{\\sum^{K}_{l=1}\\pi_{l}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{1}{2\\sigma^{2}}(x-\\mu_{l})^{2}\\right)} $$ \n",
    "\n",
    "- LDA classifier assumes that the observations within each class comes from a normal distribution with a class-specific mean vector and a common variance $\\sigma^{2}$, and pluggin estimates for these parameters into the Baye's classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Linear Discriminant Analysis for p >1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
