{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 5\n",
    "\n",
    "- [5. Linear Model Selection and Regularization](#5.-Linear-Model-Selection-and-Regularization)\n",
    "    * [5.1. Subset Selection](#5.1.-Subset-Selection)\n",
    "        + [5.1.1. Best Subset Selection](#5.1.1.-Best-Subset-Selection)\n",
    "        + [5.1.2. Stepwise Selection](#5.1.2.-Stepwise-Selection)\n",
    "        + [5.1.3. Choosing the Optimal Model](#5.1.3.-Choosing-the-Optimal-Model)\n",
    "    * [5.2. Shrinkage Methods](#5.2.-Shrinkage-Methods)\n",
    "        + [5.2.1. Ridge Regression](#5.2.1.-Ridge-Regression)\n",
    "        + [5.2.2. The Lasso](#5.2.2.-The-Lasso)\n",
    "    * [5.3. Dimension Reduction Methods](#5.3.-Dimension-Reduction-Methods)\n",
    "        + [5.3.1. Principal Components Regression](#5.3.1.-Principal-Components-Regression)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Y = \\beta_{0} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p} + \\epsilon $$\n",
    "\n",
    "- While linear model has distinct advantages in terms of inference, most problems are non-linear\n",
    "    * Alternative fitting can yield better `prediction accuracy` and `interpretability`\n",
    "\n",
    "**Prediction Accuracy**\n",
    "- Provided the true relationship between response and predictors is approximately linear, the least squares estimates will have low bias\n",
    "    * If n is >> p, then the least squares estimates will have low variance\n",
    "\n",
    "**Model Interpretability**\n",
    "- Some or many of variables used in the multiple regression model are in fact not associated with the response\n",
    "    * Including irrelevant variables leads to unnecessary complexity in the resulting model\n",
    "        + Need for feature selection or variables selection to exclude irrelevant variables\n",
    "        \n",
    "**Classes of Feature Selection**\n",
    "- Subset Selection\n",
    "    * Identifying a subset of the p predictors that we believe to be related to the response\n",
    "        + Fit the model on these reduced set of variables\n",
    "- Shrinkage\n",
    "    * Fitting a model with all p predictors\n",
    "        + But, the estimated coefficients shrunken towards zero relative to the least squares estimates to reduce variance\n",
    "        + Also known as regularization\n",
    "- Dimension Reduction\n",
    "    * Projecting the p predictors into a M-dimensional subspace where M < p\n",
    "        + Computing M different linear combinations or projections of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit a separate least squares regression for each combination of the $p$ predictors\n",
    "    * Obtain the best model from each model size according to $RSS$ or $R^{2}$\n",
    "    * For other types of models, like logistic regression, `deviance` can be used as a measure of comparison\n",
    "        + $-2\\times maximum likelihood$; smaller the deviance, the better the fit\n",
    "- Methodology:\n",
    "    * Let $M_{0}$ denote the null model with no predictors\n",
    "    * For k = 1,2, ... p :\n",
    "        + Fit all $(^{p}_{K})$ models that contain exactly k predictors\n",
    "        + Pick the best among these $(^{p}_{K})$ models, defined as the smallest RSS or largest $R^{2}$\n",
    "    * Model Selection using cross-validation prediction error\n",
    "        + $C_{p}$, AIC, BIC or adjusted $R^{2}$\n",
    "- RSS of the models decrease monotonically and the $R^{2}$ increase monotonically when the number of features included in the model increases.\n",
    "    * However, we are interested in the lowest test error rate and not the lowest training error rate\n",
    "    \n",
    "**Limitations of Best Subset Selection**\n",
    "- Becomes computationally infeasible for values of p greater than 40\n",
    "- Only work for least squares linear regression\n",
    "- The larger the search space, the higher the chance of finding models that look good on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Stepwise Selection**\n",
    "- Computationally efficient alternative to best subset selection as it only fits $1 + p(p+1)/2$ models\n",
    "- Methodology:\n",
    "    * Begin with a null model with no predictors\n",
    "    * Add predictors one-at-a-time, until all of the predictors are in the model\n",
    "    * At each set, the variables that gives the greatest additional improvement to the fit is added to the model\n",
    "    * Selection of the best model using cross validated prediction error , $C_{p}$, AIC, BIC or adjusted $R^{2}$\n",
    "- Limitations of Forward Selection\n",
    "    * Not guaranteed to find the best possible model\n",
    "\n",
    "**Backward Stepwise Selection**\n",
    "- Only searches through 1 + p(p+1)/2 models\n",
    "- Methodology:\n",
    "    * Begin with the full least squares model containing all $p$ predictors\n",
    "    * Iteratively remove the least useful predictor one-at-a-time\n",
    "        + Compare using smallest RSS or highest $R^{2}$\n",
    "    * Select a single best model using cross validated prediction error, $C_{p}$, $AIC$, $BIC$ or adjusted $R^{2}$\n",
    "- Requires that the number of samples $n$ is larger than the number of variables $p$\n",
    "\n",
    "**Hybrid Approaches**\n",
    "- After adding each new variable, the method also remove any variables that no longer provide an improvement in the model fit\n",
    "    * More closely mimic best subset selection while retaining the computational advantages of forward and backward stepwise selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3. Choosing the Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Approaches to choosing optimal model:\n",
    "    * Indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting\n",
    "    * Directly estimate the test error, using either a validation set approach or a cross-validation approach\n",
    "\n",
    "**C_{p}**\n",
    "$$ C_{p} = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^{2}) $$\n",
    "\n",
    "where $\\hat{\\sigma}^{2}$ is an estimate of the variance of the error $\\epsilon$ associated with each response measurement\n",
    "- $C_{p}$ statistic adds penalty of $2d\\hat{\\sigma}^{2}$ to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error\n",
    "\n",
    "**AIC**\n",
    "$$ AIC = \\frac{1}{n\\hat{\\sigma}^{2}}(RSS + 2d\\hat{\\sigma}^{2}) $$\n",
    "- Defined for a large class of models fit by maximum likelihood\n",
    "\n",
    "**BIC**\n",
    "$$ BIC = \\frac{1}{n\\hat{\\sigma}^{2}}(RSS + log(n)d\\hat{\\sigma}^{2}) $$\n",
    "- Derived from a Bayesian point of view\n",
    "- BIC take on a small value for a model with a low test error\n",
    "- Since log n > 2 for any n > 7, the BIC statistic places a heavier penalty on models with many variables\n",
    "\n",
    "**Adjusted $R^{2}$**\n",
    "\n",
    "$$ Adjusted R^{2} = 1 = \\frac{RSS/(n-d-1)}{TSS/(n-1)}$$\n",
    "\n",
    "where $R^{2} = 1 - RSS/TSS$ and $TSS = \\sum(y_{i}-\\bar{y})^{2}$\n",
    "- A larger value of adjusted R^{2} indicates a model with a small test error\n",
    "- Once all the correct varaibles have been included, additional variables will only lead to a very small decrease in RSS\n",
    "    * Results in a increase in $\\frac{RSS}{n-d-1}$ and consequently a decrease in the adjusted $R^{2}$\n",
    "    \n",
    "**Validation and Cross-Validation**\n",
    "- $AIC$, $BIC$, $C_{p}$ and adjusted $R^{2}$ were attractive approaches when cross-validation was computationaly prohibitive\n",
    "- Another way to check for the best model would be to use the one-standard error rule\n",
    "    * Calculate the standard error of the estimated test MSE for each model sieze\n",
    "    * Select the model for which the estimated test error is within one standard error of the lowest point on the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit a model containing all predictors using a technique that constrains and regularizes the coefficient estimates or equivalently that shrinks the coefficient estimates towards zero\n",
    "    * Shrinking the coefficient estimates can significantly reduce their variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum^{n}_{i=1}\\left(y_{i}-\\beta_{0}-\\sum^{p}_{j=1}\\beta_{j}x_{ij}\\right)^{2} + \\lambda\\sum^{p}_{j=1}\\beta^{2}_{j} = RSS + \\lambda\\sum^{p}_{j=1}\\beta^{2}_{j}$$\n",
    "where $\\lambda \\ge0$ is a tuning parameter\n",
    "- Ridge regression seeks coefficient estimates that fit the data well, by making the RSS small\n",
    "- The second term $\\lambda\\sum_{j}\\beta_{j}^2$ called a `shrinkage penalty` is small when $\\beta_{1},...,\\beta_{p}$ are close to zero\n",
    "- Tuning parameter $\\lambda$ serves to control the relative impact of these two terms on the regression coefficent\n",
    "    * When $\\lambda = 0$, the penalty terms has no effect and the ridge regression will produce the least squares estimates\n",
    "    * When $\\lambda \\rightarrow \\infty$, impact of shrinkage penalty grows and ridge regression coefficients estimates will approach zero\n",
    "        + Ridge regression produces a different set of coefficient estimates, $\\hat{\\beta_{\\lambda}^{R}}$ for each value of $\\lambda$\n",
    "        + Applied to predictors and not to the intercept\n",
    "\n",
    "**Least Squares vs Ridge Regression**\n",
    "- Least squares coefficient estimates are scale equivariant\n",
    "    * Multiplying X_{j} by a constant c simply leads to scaling of the least squares coefficient estimates\n",
    "- Ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant\n",
    "    * Scaling depends on $\\lambda$ and the $j$th predictor\n",
    "        + Best to apply ridge regression after standardizing the predictors using the formula\n",
    "$$ \\tilde{x_{ij}} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum^{n}_{i=1}(x_{ij}-\\bar{x_{j}})^{2}}}$$\n",
    "\n",
    "**Improvement of Least Squares using Ridge Regression**\n",
    "- Ridge regression's advantage lies in the *bias-variance trade-off*\n",
    "    * As $\\lambda$ increases, flexibilty of the ridge regression fit deceases, leading to decreased variance but increase bias\n",
    "    * As $\\lambda$ increases, the shrinkage of the ridge coefficent estimates leads to substantial reduction in the variance of the predictions at the expense of a slight increase in bias\n",
    "        + For $\\lambda$ up to 10, variance decrease rapidly with very little increase in bias, thus MSE drops\n",
    "- If p>n, least squares estimate do not have a uniue solution, whereas ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance\n",
    "    * Ridge regression works best in situations where the least squares estimates have high variance\n",
    "- Ridge regreesion has a computational advantage of least squares as it only fits a single model and model-fitting procedure can be performed quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantage of Ridge Regression**\n",
    "- Ridge regression will shrink all the coefficients towards zero but not set any to exactly zero (unless $\\lambda = \\infty$)\n",
    "- Creates a challenge in model interpretationwhere number of variables $p$ is quite large\n",
    "\n",
    "**Lasso Regression**\n",
    "$$ \\sum^{n}_{i=1}\\left(y_{i}-\\beta_{0}-\\sum^{p}_{j=1}\\beta_{j}x_{ij}\\right)^{2} + \\lambda\\sum^{p}_{j=1}|\\beta_{j}| = RSS + \\lambda\\sum^{p}_{j=1}|\\beta_{j}|$$\n",
    "\n",
    "where $|\\beta_{j}|$ is the lasso penalty\n",
    "\n",
    "- The differentce between ridge regression and lasso regression is that the lasso penalty $|\\beta_{j}|$ has the effect to force some of the coefficient estimates to be exactly zero when the tuning parameter $\\lambda$ is sufficiently large\n",
    "- When $|\\beta_{j}| = 0$, it will produce a least squares model but when $|\\beta_{j}|$ is large, it will produce a null model\n",
    "\n",
    "**Comparing the Lasso and Ridge Regression**\n",
    "- Advantages of Lasso over Ridge Regression\n",
    "    * More interpretable models that involve only a subset of the predictors\n",
    "    * When only a subset of predictors is truly related to the response, Lasso outperform Ridge Regression\n",
    "- Advantages of Ridge over Lasso Regression\n",
    "    * Ridge Regression will perform better when the response is a function of many predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Dimension Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduces the problem of estimating $p + 1$ coefficients to the simple problem of estimating $M + 1$ coefficients where M < p\n",
    "- Dimension reduction serves to constrain the estimated $\\beta_{j}$ coefficients \n",
    "    * However, it has the potential to bias the coefficient estimates\n",
    "- Dimension reduction works in two steps:\n",
    "    * Transformation of predictors\n",
    "    * Model fitted using these M predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Popular approach for deriving a low-dimensional set of features from a large set of variables\n",
    "\n",
    "**An Overview of Principal Components Analysis**\n",
    "- PCA is a technique for reducing the dimension of a $n \\times p$ data matrix **X**\n",
    "    * First principal component direction of the data is that along which the observations vary the most\n",
    "    \n",
    "$$ Z_{1} = \\phi_{11} \\times (X_{1} -\\bar{X_{1}}) + \\phi_{21} \\times (X_{2} -\\bar{X_{2}}) $$\n",
    "\n",
    "where $\\phi_{11}$ and $\\phi_{21}$ are principal component loadings and $Z_{1} is the principal component scores\n",
    "\n",
    "- This first principal component line minimizes the sum of squared perpendicular distances between each point and the line\n",
    "    * Projected observations are as close as possible to the originial observations\n",
    "    * Values of principal components $Z_{1}$ summarizes the joint values for each of the predictors for each location\n",
    "- The second principal component $Z_{2}$ is a linear combination of the variables that is uncorrelated with $Z_{1}$ and has the largest variance subject to this constraint\n",
    "    * The zero correlation between $Z_{1}$ and $Z_{2}$ is equivalent to the condition that the direction must be perpendicular, or orthogonal to the first principal component direction\n",
    "\n",
    "$$ Z_{2} = \\phi_{21} \\times (X_{1} -\\bar{X_{1}}) - \\phi_{11} \\times (X_{2} -\\bar{X_{2}}) $$\n",
    "\n",
    "**The Principal Components Regression Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
