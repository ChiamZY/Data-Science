{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Y = \\beta_{0} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p} + \\epsilon $$\n",
    "\n",
    "- While linear model has distinct advantages in terms of inference, most problems are non-linear\n",
    "    * Alternative fitting can yield better `prediction accuracy` and `interpretability`\n",
    "\n",
    "**Prediction Accuracy**\n",
    "- Provided the true relationship between response and predictors is approximately linear, the least squares estimates will have low bias\n",
    "    * If n is >> p, then the least squares estimates will have low variance\n",
    "\n",
    "**Model Interpretability**\n",
    "- Some or many of variables used in the multiple regression model are in fact not associated with the response\n",
    "    * Including irrelevant variables leads to unnecessary complexity in the resulting model\n",
    "        + Need for feature selection or variables selection to exclude irrelevant variables\n",
    "        \n",
    "**Classes of Feature Selection**\n",
    "- Subset Selection\n",
    "    * Identifying a subset of the p predictors that we believe to be related to the response\n",
    "        + Fit the model on these reduced set of variables\n",
    "- Shrinkage\n",
    "    * Fitting a model with all p predictors\n",
    "        + But, the estimated coefficients shrunken towards zero relative to the least squares estimates to reduce variance\n",
    "        + Also known as regularization\n",
    "- Dimension Reduction\n",
    "    * Projecting the p predictors into a M-dimensional subspace where M < p\n",
    "        + Computing M different linear combinations or projections of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1. Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit a separate least squares regression for each combination of the $p$ predictors\n",
    "    * Obtain the best model from each model size according to $RSS$ or $R^{2}$\n",
    "    * For other types of models, like logistic regression, `deviance` can be used as a measure of comparison\n",
    "        + $-2\\times maximum likelihood$; smaller the deviance, the better the fit\n",
    "- Methodology:\n",
    "    * Let $M_{0}$ denote the null model with no predictors\n",
    "    * For k = 1,2, ... p :\n",
    "        + Fit all $(^{p}_{K})$ models that contain exactly k predictors\n",
    "        + Pick the best among these $(^{p}_{K})$ models, defined as the smallest RSS or largest $R^{2}$\n",
    "    * Model Selection using cross-validation prediction error\n",
    "        + $C_{p}$, AIC, BIC or adjusted $R^{2}$\n",
    "- RSS of the models decrease monotonically and the $R^{2}$ increase monotonically when the number of features included in the model increases.\n",
    "    * However, we are interested in the lowest test error rate and not the lowest training error rate\n",
    "    \n",
    "**Limitations of Best Subset Selection**\n",
    "- Becomes computationally infeasible for values of p greater than 40\n",
    "- Only work for least squares linear regression\n",
    "- The larger the search space, the higher the chance of finding models that look good on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2. Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Stepwise Selection**\n",
    "- Computationally efficient alternative to best subset selection as it only fits $1 + p(p+1)/2$ models\n",
    "- Methodology:\n",
    "    * Begin with a null model with no predictors\n",
    "    * Add predictors one-at-a-time, until all of the predictors are in the model\n",
    "    * At each set, the variables that gives the greatest additional improvement to the fit is added to the model\n",
    "    * Selection of the best model using cross validated prediction error , $C_{p}$, AIC, BIC or adjusted $R^{2}$\n",
    "\n",
    "**Limitations of Forward Selection**\n",
    "- Not guaranteed to find the best possible model\n",
    "\n",
    "**Backward Stepwise Selection**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
