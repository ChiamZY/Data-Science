{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Learning - Chapter 4\n",
    "\n",
    "- [4. Resampling Methods](#4.-Resampling-Methods)\n",
    "    * [4.1 Cross-Validation](#4.1-Cross-Validation)\n",
    "        + [4.1.1 The Validation Set Approach](#4.1.1-The-Validation-Set-Approach)\n",
    "        + [4.1.2 Leave-One-Out Cross Validation](#4.1.2-Leave-One-Out-Cross-Validation)\n",
    "        + [4.1.3 k-Fold Cross-Validation](#4.1.3-k-Fold-Cross-Validation)\n",
    "        + [4.1.4 Bias-Variance Trade-off for k-Fold Cross-Validation](#4.1.4-Bias-Variance-Trade-off-for-k-Fold-Cross-Validation)\n",
    "        + [4.1.5 Cross-Validation on Classification Problems](#4.1.5-Cross-Validation-on-Classification-Problems)\n",
    "    * [4.2 The Bootstrap](#4.2-The-Bootstrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model\n",
    "    * Computationally expensive\n",
    "        + Fitting the same statistical model multiple times using different subsets of the training data\n",
    "- Resampling Methods\n",
    "    * Cross-validation\n",
    "        + Estimate the test error associated with a given statistical learning method in order to evaluate the model's performance **(model assessment)** \n",
    "        + Select the appropriate level of flexibility **(model selection)**\n",
    "    * Bootstrap\n",
    "        + Provide a measure of accuracy of a parameter estimate or of a given statistical learning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A simple strategy to estimate the test error associated with fitting a particular statistical learning method on a set of observations\n",
    "- Randomly dividing the available set of observations into two parts\n",
    "    * Training set\n",
    "    * Validation set\n",
    "        + Assessed using the MSE in the case of a quantitative response\n",
    "\n",
    "**Drawbacks of Validation Set Approach**\n",
    "- Validation estimate of the test error can be highly variable\n",
    "    * Dependent on the observations included in the training set and which observations are included in the validation set\n",
    "- Only a subset of the observations are used to fit the model\n",
    "    * Statistical methods tend to perform worse when trained on fewer observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Leave-One-Out Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Leave-one-out cross-validation (LOOCV) involves splitting the set of observations into two parts\n",
    "    * A single observation is used for the validation set\n",
    "    * Rest of the remaining observations make up the training set\n",
    "- Repeating this approach n times produced n squared errors\n",
    "    * LOOCV estimate for the test MSE is the average of these n test error estimates  \n",
    "\n",
    "$$ CV_{(n)} = \\frac{1}{n}\\sum^{n}_{i=1}MSE_{i} $$\n",
    "\n",
    "**Advantages and Disadvantages of LOOCV**\n",
    "- Advantages:\n",
    "    * Far less bias than validation set\n",
    "        + Repeatedly fits the statistical method using training sets that contains n-1 observations so LOOCV tends to no overestimate the test error rate\n",
    "    * LOOCV will always yield the same result as compared to validation set which will tield different results when applied repeated due to randomness in the training/validation set splits\n",
    "- Disadvantage:\n",
    "    * Potentially to be computationally expensive to implement\n",
    "        + Model has to be fitted n times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An alternative to LOOCV\n",
    "- LOOCV is a special case of k-fold CV in which k is set to equal n\n",
    "- k-fold CV involves randomly dividing the set of observations into k groups, or fold, of approximately equal size\n",
    "    * First fold is treated as a validation set and the method is fit on the remaining k-1 folds.\n",
    "    * Mean squared error  is computed on te observations in the held-out fold\n",
    "- Rpeating this k times, with a different group used as a validation set, the k-fold CV estimate is computed by averaging these values\n",
    "\n",
    "$$ CV_{(k)} = \\frac{1}{k}\\sum^{k}_{i=1}MSE_{i} $$\n",
    "\n",
    "**Advantages and Disadvantages**\n",
    "- Advatages:\n",
    "    * Less computationally intensive than LOOCV as the model is only fitted k times\n",
    "    * Variability in test error is much lower than the validation set approach\n",
    "- Disadvantage\n",
    "    * CV curves tend to obseestimate the test set MSE for higher degrees of flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Bias-Variance Trade-off for k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-fold CV gives more accurate estimates of the test error rate than LOOCV\n",
    "    * Bias comparison\n",
    "        + LOOCV gives approximately unbiased estimated estimate of the test error\n",
    "        + k-fold CB lead to an intermediate level of bias, since each training set contain $(k-1)n/k$ observations\n",
    "    * Variance comparison\n",
    "        + LOOCV has a higher variance than does k-fold CB with $k<n$\n",
    "        + In LOOCV, we are average the outputs of n fitted modesl, leading to an almost identical set of observations\n",
    "        + k-fold CV suffers neither suffers from excessively high bias nor from very high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Cross-Validation on Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For cross validation when Y is qualitative, we can use the number of misclassified observations instead of the MSE\n",
    "- The LOOCV error rate takes the form\n",
    "\n",
    "$$ CV_{(n)} = \\frac{1}{n}\\sum^{n}_{i=1}Err_{i} $$\n",
    "\n",
    "where $Err_{i} = I(y_{i} \\neq \\hat{y_{i}})$\n",
    "\n",
    "- When dealing with non-linear data:\n",
    "    * Logistic regression does not have enough flexibility to model the Bayes decision boundary\n",
    "        + Transforming to a polynomial logistic regression model to resolve this problem\n",
    "    * Training errors tend to decease as the flexibility of the fit increases an\n",
    "        + Cross validation can help determine the best model that is close to that of the true test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used to quantify the uncertainty associated with a given estimator or statistical learning method\n",
    "    * Estimate the standard errors of the coefficients\n",
    "        + Even for statistical learning methods which have variances that is difficult to obtain\n",
    "- Obtain distinct data sets by repeatedly sampling observations from the original data set with replacement\n",
    "    * Standard error of these bootstap estimates can be computed via:\n",
    "\n",
    "$$ SE_{B}(\\hat{(\\alpha)} = \\sqrt{\\frac{1}{B-1}\\sum^{B}_{r=1}\\left(\\hat{\\alpha^{*r}} - \\frac{1}{B}\\sum^{B}_{r'=1}\\hat{\\alpha^{*r'}}\\right)^{2}} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
